{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHwzFROx8jDJLpIP+yxSxw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gangaraju1411/Databricks/blob/main/Recommendations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pyspark"
      ],
      "metadata": {
        "id": "2NjpXtBk0W95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bea96182-4dfd-454f-98fa-0ef21670d5e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=126f46f3b18128af234e22aad4ef298a0a6b2b9fa7f4c0132574403e77f59d68\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L16bBO480WLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iSu89Y8tKua"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from faker import Faker\n",
        "\n",
        "# Initialize Faker for generating fake customer names and details\n",
        "fake = Faker()\n",
        "\n",
        "# Define some product names and categories\n",
        "products = [\n",
        "    {'product_id': 1, 'product_name': 'Smartphone', 'category': 'Electronics'},\n",
        "    {'product_id': 2, 'product_name': 'Laptop', 'category': 'Electronics'},\n",
        "    {'product_id': 3, 'product_name': 'Blender', 'category': 'Home Appliances'},\n",
        "    {'product_id': 4, 'product_name': 'Air Conditioner', 'category': 'Home Appliances'},\n",
        "    {'product_id': 5, 'product_name': 'Washing Machine', 'category': 'Home Appliances'},\n",
        "    {'product_id': 6, 'product_name': 'Shoes', 'category': 'Fashion'},\n",
        "    {'product_id': 7, 'product_name': 'T-shirt', 'category': 'Fashion'},\n",
        "    {'product_id': 8, 'product_name': 'Watch', 'category': 'Fashion'},\n",
        "    {'product_id': 9, 'product_name': 'Headphones', 'category': 'Electronics'},\n",
        "    {'product_id': 10, 'product_name': 'Tablet', 'category': 'Electronics'}\n",
        "]\n",
        "\n",
        "# Generate 200 customer purchase records\n",
        "data = []\n",
        "for i in range(200):\n",
        "    customer_id = random.randint(1, 50)\n",
        "    customer_name = fake.name()\n",
        "    age = random.randint(18, 65)\n",
        "    gender = random.choice(['Male', 'Female'])\n",
        "    product = random.choice(products)\n",
        "    product_id = product['product_id']\n",
        "    product_name = product['product_name']\n",
        "    purchase_amount = round(random.uniform(50, 1000), 2)\n",
        "    purchase_date = fake.date_this_year()\n",
        "    rating = random.randint(1, 5)\n",
        "    category = product['category']\n",
        "\n",
        "    data.append({\n",
        "        'customer_id': customer_id,\n",
        "        'customer_name': customer_name,\n",
        "        'age': age,\n",
        "        'gender': gender,\n",
        "        'product_id': product_id,\n",
        "        'product_name': product_name,\n",
        "        'purchase_amount': purchase_amount,\n",
        "        'purchase_date': purchase_date,\n",
        "        'rating': rating,\n",
        "        'category': category\n",
        "    })\n",
        "\n",
        "# Convert the list to a pandas DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the dataset as a CSV for further use\n",
        "df.to_csv('customer_purchase_data.csv', index=False)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(df)"
      ],
      "metadata": {
        "id": "f2oaPQIZgnEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.recommendation import ALSModel\n",
        "\n",
        "# Load the ALS model\n",
        "model_path = \"models/als_model\"\n",
        "model = ALSModel.load(model_path)\n"
      ],
      "metadata": {
        "id": "cOaCgQqVVvjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.recommendation import ALS\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"ProductRecommendation\").getOrCreate()\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('customer_purchase_data.csv')\n",
        "spark_df = spark.createDataFrame(df)\n",
        "\n",
        "# Show the dataset\n",
        "spark_df.show(5)\n",
        "\n",
        "# Prepare data for ALS (Alternating Least Squares)\n",
        "# We will use customer_id, product_id, and rating columns\n",
        "als_data = spark_df.select('customer_id', 'product_id', 'rating')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "(training, test) = als_data.randomSplit([0.8, 0.2])\n",
        "\n",
        "# Create ALS model and fit to the training data\n",
        "als = ALS(userCol=\"customer_id\", itemCol=\"product_id\", ratingCol=\"rating\", coldStartStrategy=\"drop\", nonnegative=True)\n",
        "model = als.fit(training)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.transform(test)\n",
        "\n",
        "# Evaluate the model by calculating RMSE (Root Mean Squared Error)\n",
        "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
        "\n",
        "# Generate top 5 product recommendations for each customer\n",
        "customer_recommendations = model.recommendForAllUsers(5)\n",
        "\n",
        "# Show the recommendations\n",
        "customer_recommendations.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "17UkgAtt8RsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.recommendation import ALS\n",
        "\n",
        "# Train the ALS model\n",
        "als = ALS(userCol=\"customer_id\", itemCol=\"product_id\", ratingCol=\"rating\", coldStartStrategy=\"drop\", nonnegative=True)\n",
        "model = als.fit(training)\n",
        "\n",
        "# Save the trained ALS model\n",
        "model_path = \"models/als_model\"\n",
        "model.save(model_path)\n"
      ],
      "metadata": {
        "id": "BxKTverjVhHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9KUq1QxavK7",
        "outputId": "2c038b10-b6c2-468b-d16a-e64f92ffc2f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.recommendation import ALSModel\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"ProductRecommendation\").getOrCreate()\n",
        "\n",
        "# Load the ALS model\n",
        "model_path = \"models/als_model\"  # Path where you saved the model\n",
        "model = ALSModel.load(model_path)\n",
        "\n",
        "@app.route('/recommendations', methods=['GET'])\n",
        "def get_recommendations():\n",
        "    # Get customer_id from request args\n",
        "    customer_id = request.args.get('customer_id')\n",
        "\n",
        "    if not customer_id:\n",
        "        return jsonify({\"error\": \"customer_id parameter is required\"}), 400\n",
        "\n",
        "    try:\n",
        "        # Create a DataFrame with the customer_id to get recommendations\n",
        "        user_df = spark.createDataFrame([(int(customer_id),)], [\"customer_id\"])\n",
        "\n",
        "        # Generate top 5 recommendations for the specified customer\n",
        "        recommendations = model.recommendForUserSubset(user_df, 5)\n",
        "\n",
        "        # Convert recommendations to a list of dictionaries\n",
        "        recommendations_list = []\n",
        "        for row in recommendations.collect():\n",
        "            customer_id = row['customer_id']\n",
        "            products = row['recommendations']\n",
        "            recommendations_list.append({\n",
        "                'customer_id': customer_id,\n",
        "                'products': [item['product_id'] for item in products]\n",
        "            })\n",
        "\n",
        "        return jsonify(recommendations_list)\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "# Run the Flask app with ngrok tunnel\n",
        "if __name__ == '__main__':\n",
        "    from pyngrok import ngrok\n",
        "\n",
        "\n",
        "    # Set r ngrok auth token\n",
        "    ngrok.set_auth_token('2lgUBseRGyAgeoEWcaJMlfOqjWJ_SnxETCPK9ZpzRZeiLazC')\n",
        "\n",
        "    # Start ngrok and open a tunnel to the Flask app\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(f\" * Ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:5000\\\"\")\n",
        "\n",
        "    # Start the Flask app\n",
        "    app.run(port=5000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fu1BmA5JWN34",
        "outputId": "30ad79be-71c8-43fc-a399-154030c02255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2024-09-11T09:35:10+0000 lvl=warn msg=\"can't bind default web address, trying alternatives\" obj=web addr=127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Ngrok tunnel \"NgrokTunnel: \"https://b0f8-34-66-19-152.ngrok-free.app\" -> \"http://localhost:5000\"\" -> \"http://127.0.0.1:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Sep/2024 09:35:52] \"\u001b[35m\u001b[1mGET /recommendations?customer_id=1 HTTP/1.1\u001b[0m\" 500 -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.recommendation import ALSModel\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"ProductRecommendation\").getOrCreate()\n",
        "\n",
        "# Load the ALS model\n",
        "model_path = \"models/als_model\"  # Path where you saved the model\n",
        "model = ALSModel.load(model_path)\n",
        "\n",
        "# Load the customer purchase data\n",
        "purchase_data_path = \"customer_purchase_data.csv\"  # Path to your purchase data\n",
        "purchase_data_df = spark.read.csv(purchase_data_path, header=True, inferSchema=True)\n",
        "\n",
        "# Register purchase data DataFrame as a temporary view\n",
        "purchase_data_df.createOrReplaceTempView(\"purchase_data\")\n",
        "\n",
        "@app.route('/recommendations', methods=['GET'])\n",
        "def get_recommendations():\n",
        "    # Get customer_id from request args\n",
        "    customer_id = request.args.get('customer_id')\n",
        "\n",
        "    if not customer_id:\n",
        "        return jsonify({\"error\": \"customer_id parameter is required\"}), 400\n",
        "\n",
        "    try:\n",
        "        # Create a DataFrame with the customer_id to get recommendations\n",
        "        user_df = spark.createDataFrame([(int(customer_id),)], [\"customer_id\"])\n",
        "\n",
        "        # Generate top 5 recommendations for the specified customer\n",
        "        recommendations = model.recommendForUserSubset(user_df, 5)\n",
        "\n",
        "        # Explode recommendations to get individual rows\n",
        "        recommendations_exploded = recommendations.selectExpr(\"customer_id\", \"explode(recommendations) as rec\") \\\n",
        "            .selectExpr(\"customer_id\", \"rec.product_id as product_id\")\n",
        "\n",
        "        # Get product names from the purchase data DataFrame\n",
        "        product_names_df = spark.sql(\"SELECT DISTINCT product_id, product_name FROM purchase_data\")\n",
        "\n",
        "        # Join recommendations with product names\n",
        "        recommendations_with_names = recommendations_exploded.join(\n",
        "            product_names_df,\n",
        "            on=\"product_id\",\n",
        "            how=\"left\"\n",
        "        ).select(\"customer_id\", \"product_name\")\n",
        "\n",
        "        # Convert to list of dictionaries\n",
        "        recommendations_list = recommendations_with_names \\\n",
        "            .groupBy(\"customer_id\") \\\n",
        "            .agg({\"product_name\": \"collect_list\"}) \\\n",
        "            .withColumnRenamed(\"collect_list(product_name)\", \"products\") \\\n",
        "            .toPandas() \\\n",
        "            .to_dict(orient='records')\n",
        "\n",
        "        return jsonify(recommendations_list)\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "# Run the Flask app with ngrok tunnel\n",
        "if __name__ == '__main__':\n",
        "    from pyngrok import ngrok\n",
        "\n",
        "    # Set ngrok auth token\n",
        "    ngrok.set_auth_token('2lgUBseRGyAgeoEWcaJMlfOqjWJ_SnxETCPK9ZpzRZeiLazC')\n",
        "\n",
        "    # Start ngrok and open a tunnel to the Flask app\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(f\" * Ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:5000\\\"\")\n",
        "\n",
        "    # Start the Flask app\n",
        "    app.run(port=5000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsurEkRieLCs",
        "outputId": "10413cb8-7f32-4af8-9097-47ef35462177"
      },
      "execution_count": 16,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2024-09-11T09:44:58+0000 lvl=warn msg=\"can't bind default web address, trying alternatives\" obj=web addr=127.0.0.1:4040\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Ngrok tunnel \"NgrokTunnel: \"https://d302-34-66-19-152.ngrok-free.app\" -> \"http://localhost:5000\"\" -> \"http://127.0.0.1:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Sep/2024 09:45:22] \"GET /recommendations?customer_id=1 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Sep/2024 09:45:44] \"GET /recommendations?customer_id=3 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Sep/2024 09:45:52] \"GET /recommendations?customer_id=8 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Sep/2024 09:46:06] \"GET /recommendations?customer_id=10 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Sep/2024 09:46:19] \"GET /recommendations?customer_id=20 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Sep/2024 09:46:29] \"GET /recommendations?customer_id=100 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Sep/2024 09:46:43] \"GET /recommendations?customer_id=50 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Sep/2024 09:47:05] \"GET /recommendations?customer_id=25 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Sep/2024 09:47:21] \"GET /recommendations?customer_id=24 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Sep/2024 09:47:49] \"GET /recommendations?customer_id=69 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Sep/2024 09:48:00] \"GET /recommendations?customer_id=99 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Sep/2024 09:48:21] \"GET /recommendations?customer_id=150 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Sep/2024 09:48:27] \"GET /recommendations?customer_id=50 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Sep/2024 09:48:36] \"GET /recommendations?customer_id=88 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Sep/2024 09:48:42] \"GET /recommendations?customer_id=51 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Sep/2024 09:48:47] \"GET /recommendations?customer_id=50 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Sep/2024 09:48:52] \"GET /recommendations?customer_id=49 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Sep/2024 09:48:57] \"GET /recommendations?customer_id=60 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Sep/2024 10:01:46] \"GET /recommendations?customer_id=45 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Sep/2024 10:02:05] \"GET /recommendations?customer_id=30 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Sep/2024 10:02:18] \"GET /recommendations?customer_id=3 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Sep/2024 10:02:30] \"GET /recommendations?customer_id=9 HTTP/1.1\" 200 -\n"
          ]
        }
      ]
    }
  ]
}