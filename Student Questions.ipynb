{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gangaraju1411/Databricks/blob/main/Student%20Questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install  pyngrok\n"
      ],
      "metadata": {
        "id": "VWEulUiw40S5",
        "outputId": "f48531e4-6486-489c-a4ac-6b14a5e3b17a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXnDmXR7RDr2"
      },
      "outputs": [],
      "source": [
        "from faker import Faker\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize Faker instance\n",
        "fake = Faker()\n",
        "\n",
        "# Function to generate student data\n",
        "def generate_student_data(num_records):\n",
        "    students = []\n",
        "    for _ in range(num_records):\n",
        "        student = {\n",
        "            'Name': fake.name(),\n",
        "            'Class': fake.random_element(elements=('10th', '11th', '12th', 'Undergraduate', 'Postgraduate')),\n",
        "            'Read Text': fake.text(max_nb_chars=300),  # 6-5 lines of text\n",
        "            'Phone Number': fake.phone_number()\n",
        "        }\n",
        "        students.append(student)\n",
        "    return students\n",
        "\n",
        "# Generate 50 student records\n",
        "student_data = generate_student_data(50)\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(student_data)\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv('student_data.csv', index=False)\n",
        "\n",
        "# Display the first few records\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y04m-jvKRDsJ"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load pre-trained DistilBERT model and tokenizer\n",
        "model_name = 'distilbert-base-uncased'\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Prepare dataset\n",
        "class QuizDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, questions, tokenizer, max_len):\n",
        "        self.questions = questions\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question = self.questions[idx]\n",
        "        inputs = self.tokenizer(question['text'], max_length=self.max_len, truncation=True, padding='max_length', return_tensors='pt')\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].squeeze(),\n",
        "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
        "            'labels': torch.tensor(question['label'])\n",
        "        }\n",
        "\n",
        "# Example dataset (use actual data for real training)\n",
        "questions = [{'text': 'Sample question?', 'label': 1}]\n",
        "train_data = QuizDataset(questions, tokenizer, max_len=128)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=8, shuffle=True)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    logging_dir='./logs',\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained('question_generation_model')\n",
        "tokenizer.save_pretrained('question_generation_model')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Example DataFrame for demonstration\n",
        "data = {\n",
        "    'Read Text': [\n",
        "        \"Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice, 'without pictures or conversations?'\",\n",
        "        \"The sun was setting behind the hills, casting a golden glow over the landscape. The birds sang their evening songs as the breeze gently rustled through the trees, creating a serene and peaceful atmosphere.\",\n",
        "        \"In the heart of the bustling city, a small caf√© provided a sanctuary for those seeking refuge from the chaos of urban life. The aroma of freshly brewed coffee and the soft hum of conversation created a comforting and inviting space.\",\n",
        "        \"The detective examined the crime scene carefully, noting every detail with precision. The clues were scattered around, and each one had to be analyzed thoroughly to piece together the mystery.\",\n",
        "        \"The voyage across the ocean was long and arduous, but the sight of land on the horizon filled the crew with renewed hope and excitement. They had braved the storms and challenges of the sea, and their destination was finally within reach.\"\n",
        "    ]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Generate quiz questions\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Initialize the text generation pipeline\n",
        "text_generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
        "\n",
        "def generate_quiz_questions(texts, num_questions=5):\n",
        "    questions = []\n",
        "    for text in texts:\n",
        "        # Generate questions using GPT-2\n",
        "        generated_texts = text_generator(\n",
        "            text,\n",
        "            max_length=150,   # Adjusted for reasonable length\n",
        "            max_new_tokens=100,  # Number of tokens to generate\n",
        "            num_return_sequences=num_questions,\n",
        "            truncation=True\n",
        "        )\n",
        "        for generated_text in generated_texts:\n",
        "            questions.append({'question': generated_text['generated_text'].strip(), 'choices': [], 'answer': generated_text['generated_text'].strip()})\n",
        "    return questions\n",
        "\n",
        "quiz_questions = generate_quiz_questions(df['Read Text'].tolist(), num_questions=5)\n",
        "\n",
        "# Save quiz questions to JSON\n",
        "with open('quiz_questions.json', 'w') as f:\n",
        "    json.dump(quiz_questions, f, indent=4)\n",
        "\n",
        "print(\"Quiz questions generated and saved to quiz_questions.json\")\n"
      ],
      "metadata": {
        "id": "9kvZUwy46d9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline\n",
        "import json\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Initialize the text generation pipeline\n",
        "text_generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
        "\n",
        "def generate_quiz_questions(texts, num_questions=5):\n",
        "    questions = []\n",
        "    for text in texts:\n",
        "        # Generate questions using GPT-2\n",
        "        generated_texts = text_generator(\n",
        "            text,\n",
        "            max_length=150,   # Adjusted for reasonable length\n",
        "            max_new_tokens=100,  # Number of tokens to generate\n",
        "            num_return_sequences=num_questions,\n",
        "            truncation=True\n",
        "        )\n",
        "        for generated_text in generated_texts:\n",
        "            questions.append({'question': generated_text['generated_text'].strip(), 'choices': [], 'answer': generated_text['generated_text'].strip()})\n",
        "    return questions\n",
        "\n",
        "@app.route('/generate-quiz', methods=['POST'])\n",
        "def generate_quiz():\n",
        "    data = request.json\n",
        "    if not data or 'texts' not in data:\n",
        "        return jsonify({'error': 'No texts provided'}), 400\n",
        "\n",
        "    texts = data['texts']\n",
        "    num_questions = data.get('num_questions', 5)  # Default to 5 questions if not specified\n",
        "\n",
        "    try:\n",
        "        # Generate quiz questions\n",
        "        quiz_questions = generate_quiz_questions(texts, num_questions=num_questions)\n",
        "        return jsonify(quiz_questions)\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "# Run the Flask app with ngrok tunnel\n",
        "if __name__ == '__main__':\n",
        "    from pyngrok import ngrok\n",
        "\n",
        "    # Set your ngrok auth token\n",
        "    ngrok.set_auth_token('2lgUBseRGyAgeoEWcaJMlfOqjWJ_SnxETCPK9ZpzRZeiLazC')\n",
        "\n",
        "    # Start ngrok and open a tunnel to the Flask app\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(f\" * Ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:5000\\\"\")\n",
        "\n",
        "    # Start the Flask app\n",
        "    app.run(port=5000)\n"
      ],
      "metadata": {
        "id": "IDPeoKuG65Oi",
        "outputId": "e9990a08-b256-4d2a-9ec4-154e851830c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Ngrok tunnel \"NgrokTunnel: \"https://3c8a-34-105-100-3.ngrok-free.app\" -> \"http://localhost:5000\"\" -> \"http://127.0.0.1:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [12/Sep/2024 07:03:09] \"\u001b[31m\u001b[1mPOST /generate-quiz HTTP/1.1\u001b[0m\" 400 -\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=100) and `max_length`(=3000000000000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "INFO:werkzeug:127.0.0.1 - - [12/Sep/2024 07:11:03] \"POST /generate-quiz HTTP/1.1\" 200 -\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "TensorFlow with GPU",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}